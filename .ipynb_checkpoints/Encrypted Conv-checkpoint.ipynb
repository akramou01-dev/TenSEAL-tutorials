{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd9fb3b",
   "metadata": {},
   "source": [
    "# Encrypted Convolution on MNIST \n",
    "\n",
    "In this notebook we perform encrypted eval on MNIST Dataset, and for this we will use a single Neural Network compose of 1 Convulution layer and another 2 linear layers, for simplicity we are using the square fonction as an activation fonction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb43bf",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "The model is the sequence of the below layers:\n",
    "\n",
    "- **Conv:** Convolution with 4 kernels. Shape of the kernel is 7x7. Strides are 3x3.\n",
    "- **Activation:** Square activation function.\n",
    "- **Linear Layer 1:** Input size: 256. Output size: 64.\n",
    "- **Activation:** Square activation function.\n",
    "- **Linear Layer 2:** Input size: 64. Output size: 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf8b22",
   "metadata": {},
   "source": [
    "### Convolution \n",
    "\n",
    "for the convolution operation we will use the algo that translate the 2D conv into a single matrix multiplication and \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/im2col_conv2d.png\" width=\"50%\"/>\n",
    "<div><b>Figure1:</b> Image to column convolution</div>\n",
    "</div>\n",
    "\n",
    "**The figure is taken from the official TenSEAL Tutorials**\n",
    "\n",
    "this operation requires arranging the elements of the matrix , and since we can't do that with the ciphertext so we will do a pre-processing before the encryption step.we first need to apply an *im2col* operation to the input matrix and encrypt it into a single ciphertext( we translate it into a single vecor using a vertical scan), then we do a matrix multiplication between the encrypted matrix  and the flattened kernel vector which replicate every element **n** times where **n** is the number of windows .then we do a ciphertext-plaintext multiplication witch a sequence of rotate and sum operations in order to sum the elements of a single window \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/im2col_conv2d_ckks1.png\" width=\"50%\"/>\n",
    "<div><b>Figure2:</b> Image to column convolution with CKKS - step 1</div>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/im2col_conv2d_ckks2.png\" width=\"50%\"/>\n",
    "<div><b>Figure3:</b> Image to column convolution with CKKS - step 2</div>\n",
    "</div>\n",
    "\n",
    "if we have multiple kernels so we need to do this operation multiple times and combines the results in a single vector which will be the input of the linear layer\n",
    "\n",
    "\n",
    "### Linear Layer \n",
    "for the linear layer we will multiply the vector by the plain matrix and adding the plain bias, the multiplication is used based on the method explained in the figure below : \n",
    "<div align=\"center\">\n",
    "<img src=\"assets/vec-matmul.png\" width=\"65%\"/>\n",
    "<div><b>Figure4:</b> Vector-Matrix Multiplication</div>\n",
    "</div>\n",
    "\n",
    "### Square fonction\n",
    "the square fonction is very simple we need just to multiply the vector by itself \n",
    "\n",
    "after explaining each operation we conclude that we need 6 multiplications : 2 for the convolutions, 1 for the first square fonction , 1 for the first linear layer , 1 for the second square fonctions , 1 for the second linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604b8ed",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "now that we know how these operations work in theory we will implement a model of HE using the TenSEAL lib, but first we need to implement a Pytorch Model to classify the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81497bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0741f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d868049f79f478892bcb46eb78fc9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d000f80cc5c4b05b4bfbda555c65c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0762a48d8e7a436bba76f65868f542b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8d828c6dff424785965c108dc50edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21355\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np \n",
    "\n",
    "train_data = datasets.MNIST('data',train=True,download =True,transform = transforms.ToTensor())\n",
    "test_data = datasets.MNIST('data',train=False,download=True,transform = transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10076cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dl = DataLoader(train_data,batch_size = batch_size,shuffle = True)\n",
    "test_dl = DataLoader(test_data,batch_size= batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1a1c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.440560\n",
      "Epoch: 2 \tTraining Loss: 0.154996\n",
      "Epoch: 3 \tTraining Loss: 0.107849\n",
      "Epoch: 4 \tTraining Loss: 0.083934\n",
      "Epoch: 5 \tTraining Loss: 0.068531\n",
      "Epoch: 6 \tTraining Loss: 0.058931\n",
      "Epoch: 7 \tTraining Loss: 0.051740\n",
      "Epoch: 8 \tTraining Loss: 0.046910\n",
      "Epoch: 9 \tTraining Loss: 0.041639\n",
      "Epoch: 10 \tTraining Loss: 0.037754\n"
     ]
    }
   ],
   "source": [
    "# the output of the conv2d layer will be 4 vecctors each vector contains 64 slots(because we have 64 windows 1 value for each window)\n",
    "class ConvMnist(torch.nn.Module):\n",
    "    def __init__(self, hidden=64, output=10):\n",
    "        super(ConvMnist, self).__init__()        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
    "        self.fc1 = torch.nn.Linear(256, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # the model uses the square activation function\n",
    "        x = x * x\n",
    "        # flattening while keeping the batch axis\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc1(x)\n",
    "        x = x * x\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    \n",
    "    # model in evaluation mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ConvMnist()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = train(model, train_dl, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f67ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.08376144940499217\n",
      "Class Correct : [966.0, 1124.0, 1010.0, 988.0, 963.0, 851.0, 951.0, 999.0, 957.0, 989.0]\n",
      "Class total : [980.0, 1135.0, 1032.0, 1010.0, 982.0, 892.0, 958.0, 1028.0, 974.0, 1009.0]\n",
      "Test Accuracy of 0: 98% (966/980)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 1: 99% (1124/1135)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 2: 97% (1010/1032)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 3: 97% (988/1010)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 4: 98% (963/982)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 5: 95% (851/892)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 6: 99% (951/958)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 7: 97% (999/1028)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 8: 98% (957/974)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n",
      "Test Accuracy of 9: 98% (989/1009)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9798/10000)\n"
     ]
    }
   ],
   "source": [
    "def test(model,test_dl, criterion): \n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total =list(0. for i in range(10)) \n",
    "    for data,target in test_dl: \n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        test_loss+=loss.item()\n",
    "        \n",
    "        # transform output probas to predicted class using torch.max() fonction which returns 2 results (when dim=1) : \n",
    "            # first an array with the max value of each row (the max prob in every sample class)\n",
    "            # second an array that contains the indexes of the max proba in each row\n",
    "        _,preds = torch.max(output,dim=1)\n",
    "        # preds example = [3,5,0,1,4,5,6...]\n",
    "        #compare the predictions to the true labels \n",
    "        correct = np.squeeze(preds.eq(target.data.view_as(preds)))\n",
    "        # calculate the correct labels for each object \n",
    "        for i in range(len(target)):\n",
    "            # in this loop we are going to count the number of correct prediction for avery class  \n",
    "            label = target.data[i]\n",
    "            # adding +1 to the label if the prediction is correct else adding 0 in the list defined first \n",
    "            # we add 1 to the class_correct[label] if the predictions is true(check it in the correct array) else add 0\n",
    "            class_correct[label] += correct[i].item()\n",
    "            # increment the class_total[lable] (of each label) by 1\n",
    "            class_total[label] +=1\n",
    "            \n",
    "        # calculate the avg loss test \n",
    "    test_loss /= len(target)\n",
    "    print(f\"Test loss : {test_loss}\")\n",
    "        \n",
    "    print(f\"Class Correct : {class_correct}\")\n",
    "    print(f\"Class total : {class_total}\")\n",
    "    for label in range(10):\n",
    "        print(f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})')\n",
    "\n",
    "        print(f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "            f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "        )\n",
    "    \n",
    "            \n",
    "        \n",
    "test(model,test_dl,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbfc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
